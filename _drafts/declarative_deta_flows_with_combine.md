---
layout: post
title: Declarative Data Flows with Combine - Data Rivers
---

Any non-trivial application uses some form of data, whether retrieved from a web service, from a database or generated by user interaction. A well-behaved application properly responds to data changing over time while preserving a delightful uninterrupted user experience. Achieving this result can be a challenge, though, as most applications rely on several data sources with various capabilities, and whose changes might interfere with the user experience at any time.

Reactive programming offers a neat approach to this class of problems, hiding the heterogeneous nature of data sources behind a common declarative formalism with which data from several sources can be processed, consolidated and delivered. Reactive programming is not new in the Apple ecosystem but was historically only possible with the help of third-party libraries like [ReactiveSwift](https://github.com/ReactiveCocoa/ReactiveSwift) or [RXSwift](https://github.com/ReactiveX/RxSwift). Relying on external libraries to support some programming paradigm was not a straightforward decision, though, until Apple introduced [Combine](https://developer.apple.com/documentation/combine).

This article explores a few approaches to reactive application development using Combine, illustrating how this framework can help you write better and more reliable applications. It discusses general concepts of reactive programming, before introducing a toolset letting you define data flows in an entirely declarative way.

## Data Emitters

With reactive programming you can mostly stop thinking about how data is actually retrieved and delivered. This might happen through a network request or database operation, but how the associated network or data access layers are implemented is not really important. Instead you should treat data sources in your application as opaque _data emitters_ with a well-defined API contract. This contract essentially boils down to what kind of data is emitted, how this process might fail, and whether an emitter delivers data once or continuously until possible exhaustion.

In Combine any data emitter can be exposed as a publisher, either because this is how its API was designed in the first place or because some asynchronous API call was hidden behind futures and promises. Once you have Combine data emitters the task that remains to you as a developer is to connect them in such a way that data properly flows into your models, allowing your user interface to display and respond to change correctly.

If you are using SwiftUI data changes will be reflected automatically in your user interface. But even if your application is implemented in UIKit you can still benefit from having a controlled data flow to which your user interface can respond to in a reactive manner, especially if you are using diffable data sources.

The challenge, as often, is scale. Your application probably aggregates data from several emitters. Sometimes it might even need to fetch additional data, reload everything from the beginning or receive updates automatically, for example if one of the data emitters is responsible of user data synchronization between devices. How can you cope with all possible data updates in such a way that the state of your application stays correct?

## Declarative Data Flows

Instead of erratically aggregating data from various emitters while attempting to preserve state consistency, a better approach is to have a single _controlled_ declarative data flow aggregating data from several emitters, surgically delivering consolidated data to your models. Let us start with a real-world analogy to understand what this means and which challenges this presents.

Imagine you are the owner of a warehouse storing tree logs, located at the end of a river near a lake. You are interested in receiving logs you store in your warehouse, keeping track of them in your books and on a building map. Logs themselves are sent by lumberjacks located along the river, with which you either have a long-term contract or a single-delivery arrangement. You are not interested in how the logs are produced. They might be manually chopped down or cut with a sophisticated machine, but all you need is to have them delivered at the end of the river.

This analogy can be applied to a reactive application, where logs represent data delivered to a warehouse representing a model. Books and maps can be seen as different views of the model, while lumberjacks correspond to data emitters. These emitters might deliver data once and complete, or might continue to deliver data over time. Finally, the river represents a reactive pipeline which connects all required data emitters to your model, delivering data to a single location:

// Image

In the wood business things can get more complicated, though. Some lumberjacks you are in business with might in fact secretly be merchants who have their own warehouses where other rivers connect to yours. You don't know whether these lumberjacks actually chop wood or not, and they probably have their own supply chain you cannot picture entirely. You only see the end of your river but the entire supply network might be a lot more complex, though this does not affect your business, provided logs are correctly delivered to your warehouse.

In a reactive application things are quite similar. Separate pipelines can namely connect different data emitters, forming a network built using operators like `map`, `flatMap`, `switchToLatest`, `merge` or `zip`, among others:

// Image

Imagine for example a view model providing rows of content. No matter the complexity of the process to retrieve and build these rows, all that is required in the end is a publisher of rows. Once you have such a publisher, setting up the corresponding data delivery chain is as simple as assigning its output to some property:

```swift
class ViewModel: ObservableObject {
    struct Row {
        // ...
    }
    
    @Published private(set) var rows: [Row] = []
    
    init() {
        rowPublisher()
            .receive(on: RunLoop.main)
            .assign(to: &$rows)
    }

    func rowPublisher() -> AnyPublisher<[Row], Never> {
        // ...
    }
}
```

Note that if there is no context involved, like above, `init` is where you want to set up the pipeline, as it will stay the same for the whole lifetime of the view model. If the view model requires some context, though, you should recreate the pipeline when the context changes:

```swift
class ViewModel: ObservableObject {
    struct Context {
        // ...
    }
    
    struct Row {
        // ...
    }

    @Published private(set) var rows: [Row] = []
    
    init() {}

    var context: Context {
        didSet {
            rowPublisher(for: context)
                .receive(on: RunLoop.main)
                .assign(to: &$rows)
        }
    }

    func rowPublisher(for context: Context) -> AnyPublisher<[Row], Never> {
        // ...
    }
}
```

No matter how complex it may be, a publisher implementation must describe which data emitters are involved and how their data is processed and aggregated. This is one of the benefits of using reactive programming, as you are lead to naturally define data pipelines in a _declarative_ way.

There is a challenge with this declarative approach, though. During its lifetime, a view model might namely need fresh or additional data. Sometimes it might even face delivery issues, for example due to network failure. Is a declarative approach still viable to address these special cases?

## Controllable Declarative Data Pipelines

Data emitters might only be able to deliver data in pages of results. This is usually the case with network requests, as you cannot just transmit the whole result set at once. In the example from the previous section, assuming one or several rows of content are supplied by data emitters supporting pagination, how can you load additional results?

To understand how this can be achieved let us briefly resume with our real-world analogy. In the event your warehouse stock gets depleted, you probably need to phone one or several lumberjacks to ask them for more logs, maybe bypassing their intermediaries entirely. But can you have these special orders delivered through the river? Or is it better to rely on some other channel (e.g. a truck and a road) to deliver the logs to your warehouse? Moreover, how can you ensure your warehouse does not get messy if logs come through different channels?

If you decide to receive logs simultaneously from the river and from trucks, you probably need to hire more people and have them organize to ensure correct storage and bookkeeping. For your company this comes with an associated cost and introduces new challenges and potential tracking issues.

For applications things are pretty similar. If you namely decide to deliver data through several disjoint pipelines things will get more complicated, as additional state and synchronization work is required to consolidate the results, not to mention when results might be delivered on different threads.

This is why, whether you are the owner of a log warehouse or writing a data pipeline, you surely want to avoid separate delivery channels. For applications this means you need a way to keep the same pipeline network as initially configured, while being able to ask emitters for more content as needed. This is actually possible, provided you have a way to communicate with publishers directly.

### Communicating with Publishers

Communicating with a set of publishers involves some kind of infrastructure. In our real-life example we would need a phone network with phone numbers associated to each lumberjack we want to get in contact with. In Combine a similar result can be achieved with the help of triggers and signals.

A _trigger_ is some kind of communication network with a set of _signals_, which are publishers responding to an activation request by simply emitting a `Void` value. Since they are themselves publishers, signals can be inserted into any pipeline at creation time to define control points which can be activated externally when required. By leveraging publishers we also ensure no bookkeeping or multithreading issues have to be addressed, as all the heavy lifting is entirely managed by Combine itself.

Implementing triggers and signals satisfying the above contract is actually very simple:

```swift
public struct Trigger {
    public typealias Index = Int
    public typealias Signal = AnyPublisher<Void, Never>
    
    private let sender = PassthroughSubject<Index, Never>()
    
    public init() {}
    
    public func signal(activatedBy index: Index) -> Signal {
        return sender
            .filter { $0 == index }
            .map { _ in }
            .eraseToAnyPublisher()
    }
    
    public func activate(for index: Index) {
        sender.send(index)
    }
}
```

Now equipped with a trigger:

```swift
let trigger = Trigger()
```

we can create a signal publisher associated with some identifier:

```swift
let signal = trigger.signal(activatedBy: 1)
```

and which will emit a value when activated:

```swift
trigger.activate(for: 1)
```

Note that we here use integers as identifiers, but we can simply use any hashable type with the help of the following extension:

```swift
public extension Trigger {
    func signal<T>(activatedBy t: T) -> Signal where T: Hashable {
        return signal(activatedBy: t.hashValue)
    }

    func activate<T>(for t: T) where T: Hashable {
        activate(for: t.hashValue)
    }
}
```

Now that are equpped with a mechanism to communicate with publishers, let us discuss how a data emitter API can be upgraded to respond to external on-demand requests.

### Triggerable Data Emitters

- Publisher signature vs paginated publisher signature (Signal parameter)
- Two use of signals: Wait on signal / Recreate on signal

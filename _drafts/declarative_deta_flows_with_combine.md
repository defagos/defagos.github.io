---
layout: post
title: Declarative Data Flows with Combine - Data Rivers
---

Any non-trivial application uses some form of data, whether retrieved from a web service, from a database or generated by user interaction. A well-behaved application properly responds to data changing over time while preserving a delightful uninterrupted user experience. Achieving this result can be a challenge, though, as most applications rely on several data sources with various capabilities, and whose changes might interfere with the user experience at any time.

Reactive programming offers a neat approach to this class of problems, hiding the heterogeneous nature of data sources behind a common declarative formalism with which data from several sources can be processed, consolidated and delivered. Reactive programming is not new in the Apple ecosystem but was historically only possible with the help of third-party libraries like [ReactiveSwift](https://github.com/ReactiveCocoa/ReactiveSwift) or [RXSwift](https://github.com/ReactiveX/RxSwift). Relying on external libraries to support some programming paradigm was not an easy decision, though, until Apple introduced [Combine](https://developer.apple.com/documentation/combine).

This article explores a few approaches to reactive application development using Combine, illustrating how this framework can help you write better and more reliable applications. It discusses general concepts of reactive programming, before introducing a toolset letting you define data flows in an entirely declarative way.

## Data Emitters

With reactive programming you can mostly stop thinking about how data is actually retrieved and delivered. This might happen through a network request or database operation, but how the associated network or data access layers are implemented is not really important. Instead you should treat data sources in your application as opaque _data emitters_ with a well-defined API contract. This contract essentially boils down to what kind of data is emitted, how this process might fail, and whether an emitter delivers data once or continuously until possible exhaustion.

In Combine any data emitter can be exposed as a publisher, either because this is how its API was designed in the first place or because some asynchronous API call was hidden behind futures and promises. Once you have Combine data emitters the task that remains to you as a developer is to connect them in such a way that data properly flows into your models, allowing your user interface to display and respond to change correctly.

If you are using SwiftUI data changes will be reflected automatically in your user interface. But even if your application is implemented in UIKit you can still benefit from having a controlled data flow to which your user interface can respond to in a reactive manner, especially if you are using diffable data sources.

The challenge, as often, is scale. Your application probably aggregates data from several emitters. Sometimes it might even need to fetch additional data, reload everything from the beginning or receive updates automatically, for example if one of the data emitters is responsible of user data synchronization between devices. How can you cope with all possible data updates in such a way that the state of your application stays correct?

## Declarative Data Flows

Instead of erratically aggregating data from various emitters while attempting to preserve state consistency, a better approach is to have a single controlled declarative data flow aggregating data from several emitters, surgically delivering consolidated data to your models. Let us start with a real-world analogy to understand what this means and which challenges this presents.

Imagine you are the owner of a warehouse storing tree logs, located at the end of a river near a lake. You are interested in receiving logs you store in your warehouse, keeping track of them in your books and on a building map. Logs themselves are sent by lumberjacks located along the river, with which you either have a long-term contract or a single-delivery arrangement. You are not interested in how the logs are produced. They might be manually chopped down or cut with a sophisticated machine, but all you need is to have them delivered at the end of the river.

This analogy can be applied to a reactive application, where logs represent data delivered to a warehouse representing a model. Books and maps can be seen as different views of the model, while lumberjacks correspond to data emitters. These emitters might deliver data once and complete, or might continue to deliver data over time. Finally, the river represents a reactive pipeline which connects all required data emitters to your model, delivering data to a single location:

// Image

In the wood business things can get more complicated, though. Some lumberjacks you are in business with might in fact secretly be merchants who have their own warehouses where other rivers connect to yours. You don't know whether these lumberjacks actually chop wood or not, and they probably have their own supply chain you cannot picture entirely. You only see the end of your river but the entire supply network might be a lot more complex, though this does not affect your business, provided logs are correctly delivered to your warehouse.

In a reactive application things are quite similar. Separate pipelines can namely connect different data emitters, forming a network built using operators like `map`, `flatMap`, `switchToLatest`, `merge` or `zip`, among others:

// Image

Imagine for example a view model providing rows of content. No matter the complexity of the process to retrieve and build these rows, all that is required in the end is a publisher of rows. Once you have such a publisher, setting up the corresponding data delivery chain is as simple as assigning its output to some property:

```swift
class ViewModel: ObservableObject {
    struct Row {
        // ...
    }
    
    @Published private(set) var rows: [Row] = []
    
    init() {
        rowsPublisher()
            .receive(on: RunLoop.main)
            .assign(to: &$rows)
    }

    func rowsPublisher() -> AnyPublisher<[Row], Never> {
        // ...
    }
}
```

Note that if there is no context involved, like above, `init` is where you want to set up the pipeline, as it will stay the same for the whole lifetime of the view model. If the view model requires some context, though, you should recreate the pipeline when the context changes:

```swift
class ViewModel: ObservableObject {
    struct Context {
        // ...
    }
    
    struct Row {
        // ...
    }

    @Published private(set) var rows: [Row] = []
    
    init() {}

    var context: Context {
        didSet {
            rowsPublisher(for: context)
                .receive(on: RunLoop.main)
                .assign(to: &$rows)
        }
    }

    func rowsPublisher(for context: Context) -> AnyPublisher<[Row], Never> {
        // ...
    }
}
```

Updates of the pipeline might only occur when the context changes in the second case, otherwise the pipeline setup stays the same. The pipeline setups above are therefore _declarative_, describing which data emitters are involved, how data they emit is processed and how it reaches your model. This approach is especially neat, as all complicated work (asynchronous API calls with possibly different approaches, error management and synchronization issues) is entirely managed by the pipeline itself.

There is a challenge with such a declarative approach, though. During its lifetime a view model might namely need fresh or additional data from some emitters. Sometimes it might even face delivery issues, for example due to network failure. Is a declarative approach still viable to address these scenarios?

## Pipeline Control

Data emitters might only be able to deliver data in pages of results. This is usually the case with network requests, as you cannot just transmit the whole result set at once. In the example from the previous section, assuming one or several rows of content are supplied by data emitters supporting pagination, how can you load additional results?

To understand how this can be achieved let us briefly resume with our real-world analogy. In the event your warehouse stock gets depleted, you probably need to phone one or several lumberjacks to ask them for more logs, maybe bypassing their intermediaries entirely. But can you have these special orders delivered through the river? Or is it better to rely on some other channel (e.g. a truck and a road) to deliver the logs to your warehouse? Moreover, how can you ensure your warehouse does not get messy if logs come through different channels?

If you decide to receive logs simultaneously from the river and from trucks, you probably need to hire more people and have them organize to ensure correct storage and bookkeeping. For your company this comes with an associated cost and introduces new challenges and potential tracking issues.

For applications things are pretty similar. If you namely decide to deliver data through several disjoint pipelines things will get messier, as additional state and synchronization work is required to consolidate the results, not to mention when these might be delivered on different threads.

This is why, whether you are the owner of a log warehouse or writing a data pipeline, you surely want to avoid separate delivery channels. For declarative pipelines this means you need a way to keep the same pipeline network as initially configured, while being able to ask emitters for more content as needed. This is actually possible, provided you have a way to communicate with publishers directly.

### Communicating with Publishers

Communicating with a set of publishers involves some kind of infrastructure. In our real-life example we would need a phone network with phone numbers associated to each lumberjack we want to get in contact with. In Combine a similar result can be achieved with the help of triggers and signals.

A _trigger_ is some kind of communication network with a set of _signals_, which are publishers responding to an activation request by simply emitting a `Void` value. Since they are themselves publishers, signals can be inserted into any pipeline at creation time to define control points which can be activated externally when required. By leveraging publishers we also ensure no bookkeeping or multithreading issues have to be addressed, as all the heavy lifting is entirely managed by Combine itself.

Triggers and signals are actually simple to implement:

```swift
public struct Trigger {
    public typealias Index = Int
    public typealias Signal = AnyPublisher<Void, Never>
    
    private let sender = PassthroughSubject<Index, Never>()
    
    public init() {}
    
    public func signal(activatedBy index: Index) -> Signal {
        return sender
            .filter { $0 == index }
            .map { _ in }
            .eraseToAnyPublisher()
    }
    
    public func activate(for index: Index) {
        sender.send(index)
    }
}
```

Equipped with a trigger:

```swift
let trigger = Trigger()
```

we can create a signal publisher associated with some identifier:

```swift
let signal = trigger.signal(activatedBy: 1)
```

and which will emit a value when activated:

```swift
trigger.activate(for: 1)
```

Note that we use integers as identifiers, but we can simply use any hashable type with the help of the following extension:

```swift
public extension Trigger {
    func signal<T>(activatedBy t: T) -> Signal where T: Hashable {
        return signal(activatedBy: t.hashValue)
    }

    func activate<T>(for t: T) where T: Hashable {
        activate(for: t.hashValue)
    }
}
```

Now that we have a mechanism to communicate with publishers, let us discuss how a data emitter API can be upgraded to respond to external on-demand requests.

### Controlling Data Emitters with Triggers

Imagine you have some data emitter publishing items:

```swift
func itemsPublisher() -> AnyPublisher<[Item], Error>
```

This publisher might be part of a complex data pipeline created in a declarative way. If this data emitter is able to emit pages of results, we optionally want to be able to ask for a next page of results when needed. This becomes possible if we associate it with a corresponding optional signal:

```swift
func itemsPublisher(paginatedBy paginator: Trigger.Signal? = nil) -> AnyPublisher<[Item], Error>
```

Once the publisher delivered a page of result we want to put them in a dormant state, only woken up using our signal when the next page of results is needed. We only need a way to make the item publisher wait in between pages, which is easy to achieve by prepending the item pipeline with another one controlled with the signal:

```swift
public extension Publisher {
    func wait<S>(untilOutputFrom signal: S) -> AnyPublisher<Self.Output, Self.Failure> where S: Publisher, S.Failure == Never {
        return self
            .prepend(
                Empty(completeImmediately: false)
                    .prefix(untilOutputFrom: signal)
            )
            .eraseToAnyPublisher()
    }
}
```

Our implementation must only have access to an emitter able to publish items for some page `Page` (first page if `nil`), optionally returning a next page if any is available, or `nil` if all pages have been exhausted:

```swift
typealias ItemPage = (items: [Items], next: Page?)

func pageItemsPublisher(for page: Page? = nil, paginatedBy paginator: Trigger.Signal? = nil) -> AnyPublisher<ItemPage, Error>
```

Using this internal publisher helper we can implement the paginated triggerable above with a recursion:

```swift
func itemsPublisher(paginatedBy paginator: Trigger.Signal? = nil) -> AnyPublisher<[Item], Error> {
    return pageItemsPublisher()
        .map { itemPage -> AnyPublisher<[Item], Error> in
            if let paginator = paginator, let page = itemPage.next {
                return pageItemsPublisher(for: itemPage.page, paginatedBy: paginator)
                    .map { $0.items }
                    .wait(untilOutputFrom: signal)
                    .retry(.max)
                    .prepend(itemPage.items)
                    .eraseToAnyPublisher()
            }
            else {
                return Just(itemPage.items)
                    .setFailureType(to: Error.self)     // Only required for iOS 13 compatibility
                    .eraseToAnyPublisher()
            }
        }
        .switchToLatest()
}
```







- Publisher signature vs paginated publisher signature (Signal parameter)
- Two use of signals: Wait on signal / Recreate on signal
- One page at a time, collect higher
